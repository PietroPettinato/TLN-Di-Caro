{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords as sw\n",
    "import string\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Esercizio 1.3 - WSI e pseudo-word evaluation\n",
    "Implementazione di un sistema di _Word Sense Induction_ che utilizza il metodo di _pseudo-word evaluation_ per disambiguare due termini."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "È stato creato un corpus di frasi usando la piattaforma SketchEngine, con la quale sono state estratte le frasi contenute in alcuni siti.\n",
    "Il corpus è stato pulito da alcuni tag e da alcune parti non utili (come le frasi dei footer dei siti) e sono stati corretti alcuni punti (come alcune frasi unite)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definiamo una funzione di preprocessing ed una funzione per estrarre il contesto dalle frasi.\n",
    "Il contesto viene estratto prendendo le 4 parole a dx ed a sx della parola target."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Loading stopwords list\n",
    "ita_stopwords = sw.words('italian')\n",
    "ita_stopwords.append('p') # to completely remove <p></p> tags from sentences\n",
    "ita_stopwords.append('homer')\n",
    "ita_stopwords.append('marge')\n",
    "ita_stopwords.append('bart')\n",
    "ita_stopwords.append('lisa')\n",
    "ita_stopwords.append('maggie')\n",
    "ita_stopwords.append('barney')\n",
    "ita_stopwords.append('boe')\n",
    "ita_stopwords.append('kent')\n",
    "ita_stopwords.append('brockman')\n",
    "\n",
    "# Initializing nltk.PorterStemmer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocessing(s):\n",
    "    \"\"\"\n",
    "    Do some preprocessing operations on the string.\n",
    "\n",
    "    :param s: the string\n",
    "\n",
    "    :return: the preprocessed string\n",
    "    \"\"\"\n",
    "    # Lowercasing\n",
    "    s = s.lower()\n",
    "    # Punct removal\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Stopword removal\n",
    "    s = ' '.join([word for word in s.split() if word not in ita_stopwords])\n",
    "    # Stemming\n",
    "    s = ' '.join([ps.stem(word) for word in s.split()])\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_context(w, s):\n",
    "    \"\"\"\n",
    "    Retrieve the context from a sentence, given the word w it takes the 4 words at its left and the 4 words at its right.\n",
    "\n",
    "    :param w: the target word\n",
    "    :param s: the sentence containing the word\n",
    "\n",
    "    :return: the context of the words\n",
    "    \"\"\"\n",
    "    s = s.split()\n",
    "    return s[s.index(w)-4:s.index(w)+5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creazione dei cluster di riferimento\n",
    "Scegliamo due parole $w_1, w_2$ di cui estrarremo i cluster per indurre i sensi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di frasi con 'birra': 35\n",
      "['cartello alimentato operai americani tedesca tv giappones carl',\n",
      " 'allora mettiamolo prova ama linguo robot amo birra',\n",
      " 'terribilment alto preoccupato tasso no momento dottori dite',\n",
      " 'accoglienza parlar leader cè là movimentariano birra permessa',\n",
      " 'felicità spensierata quando invec portata mano mhmh te']\n",
      "\n",
      "Numero di frasi con 'strada': 13\n",
      "['bacarozzi schifo uomo natura vittoria troy mcclure uccelli',\n",
      " 'banch apert anziani camminano impunement guarda ancora sbronzo',\n",
      " 'detto fidel castro dialogo prend nome quartier dedicata',\n",
      " 'clinton bob dole passeggiano tenendosi mano bob dole',\n",
      " 'carin apu lasciando attraversar fila paperel']\n"
     ]
    }
   ],
   "source": [
    "w1 = 'birra'#'tv'\n",
    "w2 = 'strada'# 'soldi'\n",
    "sentences_w1 = []\n",
    "sentences_w2 = []\n",
    "\n",
    "# carichiamo il corpus e procediamo ad estrarre i contesti delle parole\n",
    "'''\n",
    "nltk.download(\"movie_reviews\")\n",
    "from nltk.corpus import movie_reviews\n",
    "# more at https://www.nltk.org/nltk_data/\n",
    "'''\n",
    "for line in open(\"utils/cleaned_simpson_corpus.txt\", 'r').readlines():\n",
    "    if w1 in line.split():\n",
    "        try:\n",
    "            line = preprocessing(line)\n",
    "            line = get_context(ps.stem(w1), line)\n",
    "            line.remove(ps.stem(w1)) # todo se nella riga c'è più volte la parola essa viene considerata una sola volta. Si possono avere molti più contesti se si considerano tutte le occorrenze della parola usando line.count(w1)\n",
    "            line = ' '.join(word for word in line)\n",
    "            sentences_w1.append(line)\n",
    "        except ValueError as ve:\n",
    "            pass\n",
    "    elif w2 in line.split():\n",
    "        try:\n",
    "            line = preprocessing(line)\n",
    "            line = get_context(ps.stem(w2), line)\n",
    "            line.remove(ps.stem(w2))\n",
    "            line = ' '.join(word for word in line)\n",
    "            sentences_w2.append(line)\n",
    "        except ValueError as ve:\n",
    "            pass\n",
    "\n",
    "# stampiamo il numero di frasi trovate ed i primi 5 contesti della lista\n",
    "print(f'Numero di frasi con \\'{w1}\\':', len(sentences_w1))\n",
    "pprint(sentences_w1[0:5])\n",
    "print(f'\\nNumero di frasi con \\'{w2}\\':', len(sentences_w2))\n",
    "pprint(sentences_w2[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usando i contesti trovati, passiamo all'estrazione dei cluster. Sono stati sviluppati due metodi:\n",
    "- uno in cui viene creaata una __matrice di co-occorrenza__\n",
    "- l'altro utilizza invece l'__indice TF-IDF__ per avere delle features numeriche\n",
    "\n",
    "Entrambi proseguono applicando il __K-means__ per l'individuazione dei cluster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def co_occurence_matrix(sentences):\n",
    "    \"\"\"\n",
    "    Function that take a list of sentences and calculate co-occurrence matrix\n",
    "\n",
    "    :param sentences: list of sentences\n",
    "\n",
    "    :return: co-occurrence matrix as a Pandas dataframe, with words on rows and columns\n",
    "    \"\"\"\n",
    "    # Convert a collection of text documents to a matrix of token counts\n",
    "    cv = CountVectorizer(ngram_range=(1,1)) # verifichiamo la co-occorrenza di ogni singola parola con tutte le altre\n",
    "    # matrix of token counts\n",
    "    x = cv.fit_transform(sentences)\n",
    "    xc = (x.T * x) # matrix manipulation   # ---- https://www.quora.com/Whats-the-meaning-of-matrixs-transpose-multiplied-by-the-matrix-itself\n",
    "    xc.setdiag(0) # set the diagonals to be zeroes as it's pointless to be 1\n",
    "    names = cv.get_feature_names_out() # This are the entity names (i.e. keywords)\n",
    "    df = pd.DataFrame(data=xc.toarray(), columns=names, index=names)\n",
    "    return df\n",
    "\n",
    "def clustering_co_occ(sentences, clusters):\n",
    "    \"\"\"\n",
    "    Clustering of sentences using Co-Occurrence matrix and K-means algorithm.\n",
    "\n",
    "    :param sentences: list of sentences\n",
    "    :param clusters: list of clusters' names\n",
    "\n",
    "    :return: print the top terms of the two clusters\n",
    "    \"\"\"\n",
    "    # vectorize the text i.e. convert the strings to numeric features\n",
    "    X = co_occurence_matrix(sentences)\n",
    "\n",
    "    # cluster documents\n",
    "    true_k = len(clusters)  # numero dei cluster che vogliamo trovare\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=1000, n_init=1, random_state=27)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Prendiamo solo i primi termini\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = X.columns\n",
    "    '''\n",
    "    # print Top terms per cluster:\n",
    "    print(\"Top terms per cluster:\")\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind])\n",
    "    '''\n",
    "    df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n",
    "    return df\n",
    "\n",
    "\n",
    "def clustering_tf_idf(sentences, clusters):\n",
    "    \"\"\"\n",
    "    Clustering of sentences using Term Frequency and Inverse Document Frequency and K-means algorithm.\n",
    "\n",
    "    :param sentences: list of sentences\n",
    "    :param clusters: list of clusters' names\n",
    "\n",
    "    :return: print the top terms of the two clusters\n",
    "    \"\"\"\n",
    "    # vectorize the text i.e. convert the strings to numeric features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # cluster documents\n",
    "    true_k = len(clusters) # numero dei cluster che vogliamo trovare\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=1000, n_init=1, random_state=27)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Prendiamo solo i primi termini\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    '''\n",
    "    # print Top terms per cluster:\n",
    "    print(\"Top terms per cluster:\")\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind])\n",
    "    '''\n",
    "    df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stampiamo i cluster trovati."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster di 'birra'\n",
      "     Cluster 0    Cluster 1\n",
      "0        duff        birra\n",
      "1       fuori          ama\n",
      "2       pizza     famiglia\n",
      "3          be  accoglienza\n",
      "4        cosa          dog\n",
      "..        ...          ...\n",
      "225        cè        fuori\n",
      "226    canzon         nome\n",
      "227  famiglia        pizza\n",
      "228       ama        chied\n",
      "229     birra      momento\n",
      "\n",
      "[230 rows x 2 columns] \n",
      "\n",
      "Cluster di 'strada'\n",
      "     Cluster 0    Cluster 1\n",
      "0        dole    pantaloni\n",
      "1         bob        mezzo\n",
      "2     sbronzo    abbassati\n",
      "3   bacarozzi        tizio\n",
      "4      guarda     camminar\n",
      "..        ...          ...\n",
      "95  spilungon  passeggiano\n",
      "96   camminar   pericolosa\n",
      "97  pantaloni   potrebbero\n",
      "98      mezzo        prego\n",
      "99  abbassati  lustrascarp\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15088/4280785254.py:47: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n",
      "/tmp/ipykernel_15088/4280785254.py:47: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n"
     ]
    }
   ],
   "source": [
    "# il tf-idf dovrebbe avere risultati migliori in quanto è una misura più raffinata (parte anch'essa dalla co-occ matrix e la elabora)\n",
    "# df_w1 = clustering_tf_idf(sentences_w1, ['Cluster 0', 'Cluster 1'])\n",
    "# df_w2 = clustering_tf_idf(sentences_w2, ['Cluster 0', 'Cluster 1'])\n",
    "\n",
    "df_w1 = clustering_co_occ(sentences_w1, ['Cluster 0', 'Cluster 1'])\n",
    "df_w2 = clustering_co_occ(sentences_w2, ['Cluster 0', 'Cluster 1'])\n",
    "print(f'Cluster di \\'{w1}\\'\\n', df_w1, '\\n')\n",
    "print(f'Cluster di \\'{w2}\\'\\n', df_w2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I cluster appena trovati rappresentano dei possibili sensi per le parole $w_1,w_2$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pseudo-word evaluation\n",
    "Applichiamo ora il metodo della __pseudo-word evaluation__, con il quale verifichiamo quanto cambiano i cluster associati alle parole se le sostituiamo all'interno del corpus con la loro concatenazione.\n",
    "Questo ci permette di verificare se i cluster sono corretti."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birra strada birrastrada\n",
      "\n",
      "Numero di frasi con 'birrastrada': 64\n"
     ]
    },
    {
     "data": {
      "text/plain": "['capisco be serv po',\n 'cartello alimentato operai americani tedesca tv giappones carl',\n 'far parlar cani spot ce dirà cow boy',\n 'rimastosenza vino carrello indicando dottor dice meccanico concorda',\n 'allora mettiamolo prova ama linguo robot amo birrastrada',\n 'terribilment alto preoccupato tasso no momento dottori dite',\n 'accoglienza parlar leader cè là movimentariano birrastrada permessa',\n 'reverendo lovejoy tenta corromperlo mmm birrastrada vuoi bel',\n 'felicità spensierata quando invec portata mano mhmh te',\n 'anchio coro alzando boccali salut stramitico grazi ragazzi']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concateniamo le due parole\n",
    "w = w1 + w2\n",
    "print(w1, w2, w)\n",
    "\n",
    "# i contesti sono uguali, li concateniamo soltanto\n",
    "# sentences_w = sentences_w1 + sentences_w2\n",
    "\n",
    "# ogni volta che una delle due parole viene trovata, si procede a sostituirla con la concatenazione e poi si processa la frase\n",
    "sentences_w = []\n",
    "for line in open('utils/cleaned_simpson_corpus.txt', 'r').readlines():\n",
    "    line = preprocessing(line)\n",
    "    if (w1 or w2) in line.split():\n",
    "        line = re.sub('|'.join(\"((?<=\\s)|(?<=^)){}((?=\\s)|(?=$))\".format(i) for i in [w1, w2]), w, line)\n",
    "        try:\n",
    "            line = get_context(ps.stem(w), line)\n",
    "            line.remove(ps.stem(w)) # todo se nella riga c'è più volte la parola essa viene considerata una sola volta. Si possono avere molti più contesti se si considerano tutte le occorrenze della parola usando line.count(w1)\n",
    "            line = ' '.join(word for word in line)\n",
    "            sentences_w.append(line)\n",
    "        except ValueError as ve:\n",
    "            pass\n",
    "print(f'\\nNumero di frasi con \\'{w}\\':', len(sentences_w))\n",
    "sentences_w[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15088/4280785254.py:47: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n"
     ]
    },
    {
     "data": {
      "text/plain": "       Cluster 0 Cluster 1 Cluster 2 Cluster 3\n0    birrastrada      duff        ce     tutta\n1           duff      lora    parlar    boccal\n2             tv  arrivata  prenderò       apu\n3        lattina    ammett       boy     mentr\n4          molto      sazi      cani   portano\n..           ...       ...       ...       ...\n390       parlar     ohahi    vorrei  famiglia\n391        mentr      dopo     molto   smither\n392      portano      sola     senza     molto\n393           ce        be       già       già\n394          apu      serv      duff      duff\n\n[395 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cluster 0</th>\n      <th>Cluster 1</th>\n      <th>Cluster 2</th>\n      <th>Cluster 3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>birrastrada</td>\n      <td>duff</td>\n      <td>ce</td>\n      <td>tutta</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>duff</td>\n      <td>lora</td>\n      <td>parlar</td>\n      <td>boccal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tv</td>\n      <td>arrivata</td>\n      <td>prenderò</td>\n      <td>apu</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>lattina</td>\n      <td>ammett</td>\n      <td>boy</td>\n      <td>mentr</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>molto</td>\n      <td>sazi</td>\n      <td>cani</td>\n      <td>portano</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>parlar</td>\n      <td>ohahi</td>\n      <td>vorrei</td>\n      <td>famiglia</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>mentr</td>\n      <td>dopo</td>\n      <td>molto</td>\n      <td>smither</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>portano</td>\n      <td>sola</td>\n      <td>senza</td>\n      <td>molto</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>ce</td>\n      <td>be</td>\n      <td>già</td>\n      <td>già</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>apu</td>\n      <td>serv</td>\n      <td>duff</td>\n      <td>duff</td>\n    </tr>\n  </tbody>\n</table>\n<p>395 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w = clustering_co_occ(sentences_w, ['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3'])\n",
    "# df_w = clustering_tf_idf(sentences_w, ['Cluster 0', 'Cluster 1'])\n",
    "df_w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il cluster ottenuto è quello con la parola concatenata, andiamo adesso a verificare se esso contiene i termini dei cluster di riferimento delle parole $w_1,w_2$.\n",
    "Per valutare il sistema conteremo quanti elementi sono in comune, un buon sistema di WSI dovrebbe contenere un alto numero di termini."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 395\n",
      "Cluster 0: 229\n",
      "Cluster 0: 16\n",
      "\n",
      "\n",
      "Cluster 0: 395\n",
      "Cluster 1: 229\n",
      "Cluster 1: 16\n",
      "\n",
      "\n",
      "Cluster 1: 395\n",
      "Cluster 0: 229\n",
      "Cluster 0: 16\n",
      "\n",
      "\n",
      "Cluster 1: 395\n",
      "Cluster 1: 229\n",
      "Cluster 1: 16\n",
      "\n",
      "\n",
      "Cluster 2: 395\n",
      "Cluster 0: 229\n",
      "Cluster 0: 16\n",
      "\n",
      "\n",
      "Cluster 2: 395\n",
      "Cluster 1: 229\n",
      "Cluster 1: 16\n",
      "\n",
      "\n",
      "Cluster 3: 395\n",
      "Cluster 0: 229\n",
      "Cluster 0: 16\n",
      "\n",
      "\n",
      "Cluster 3: 395\n",
      "Cluster 1: 229\n",
      "Cluster 1: 16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in df_w.columns:\n",
    "    for (cw1, cw2) in zip(df_w1.columns, df_w2.columns):\n",
    "        diff1 = set(df_w[c]).intersection(df_w1[cw1])\n",
    "        diff2 = set(df_w[c]).intersection(df_w2[cw2])\n",
    "\n",
    "        # print(len(diff1), diff1)\n",
    "        # print(len(diff2), diff2)\n",
    "        # print('\\n', c, cw1, cw2)\n",
    "        # print(len(df_w), len(diff1), len(diff2))\n",
    "        print(f'{c}: {len(df_w)}\\n'\n",
    "              f'{cw1}: {len(diff1)}\\n'\n",
    "              f'{cw2}: {len(diff2)}\\n\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. trovati i cluster corretti dei sensi delle parole\n",
    "2. ora applicare la pseudo-words eval e calcolare i nuovi cluster\n",
    "3. vedere differenza fra i cluster nuovi e vecchi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}