{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.wsd import lesk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Esercizio 1.4 - Teoria di Hanks\n",
    "\n",
    "La Teoria di Hanks propone di guardare al verbo ed alla sua arità per trovare il significato di una frase.\n",
    "Prevede un processo composto di tre passi:\n",
    "1. estrazione dei _fillers_ del verbo\n",
    "2. recupero dei _semantic types_ a partire dai _fillers_\n",
    "3. clusterizzazione dei _semantic types_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per questo esercizio è stato utilizzato il corpus _movie reviews_ di NLTK, contenente recensioni di film."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Scegliamo il verbo su cui operare\n",
    "VERB = 'get'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estrazione dei fillers del verbo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dal corpus recuperiamo 1000 frasi contenenti il verbo _get_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    },
    {
     "data": {
      "text/plain": "array(['they get into an accident .',\n       \"now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem .\",\n       'okay , we get it .',\n       \"you're more likely to get a kick out of her work in halloween h20 .\",\n       'all this to get to the familiar yet offensive \" revelation \" that sexual deviants are not , indeed , monsters but everyday people like you and me .'],\n      dtype='<U887')"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentences(verb, corpus, num):\n",
    "    \"\"\"\n",
    "    Given a corpus and a verb return the number of senteneces specified in the limit.\n",
    "\n",
    "    :param verb: the verb as a string\n",
    "    :param corpus: the corpus in which the search must be done\n",
    "    :param num: the number of sentences to retrieve\n",
    "\n",
    "    :return: the list of sentences containing the verb\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    sentences = np.array([])\n",
    "    while len(sentences) < num:\n",
    "        review = corpus.raw(corpus.fileids()[i])\n",
    "        if verb in review.split():\n",
    "            for s in sent_tokenize(review): # le recensioni contengono più frasi, le dividiamo considerandole singolarmente\n",
    "                if verb in s.split():\n",
    "                    sentences = np.append(sentences, s)\n",
    "        i += 1\n",
    "    return sentences\n",
    "\n",
    "\n",
    "sentences = get_sentences(VERB, movie_reviews, 1000)\n",
    "print(len(sentences))\n",
    "sentences[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Effettuiamo __parsing__ e __disambiguazione__."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Partiamo dal __parsing__, il cui fine è quello di estrarre i _fillers_ del verbo dalle frasi.\n",
    "Essendo il verbo scelto un verbo transitivo, consideriamo l'arità pari a 2 (soggetto ed oggetto), otteniamo così delle triple _(subj, verb, obj)_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di triple estratte: 266\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('we', 'get', 'it'),\n ('that', 'get', 'laughs'),\n ('who', 'get', 'kicks'),\n ('we', 'get', 'money'),\n ('movie', 'get', 'star')]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # usiamo la libreria SpaCy per ottenere le dipendenze delle frasi\n",
    "fillers = [] # inseriremo qui il soggetto e l'oggetto del verbo\n",
    "index = [] # teniamo traccia dell'indice della frase per poterla recuperare nelle operazioni successive (WSD)\n",
    "for i in range(len(sentences)):\n",
    "    doc = nlp(str(sentences[i]))\n",
    "    df = pd.DataFrame(data=[(token.dep_, token.head, [c for c in token.children]) for token in doc], index=[token.text for token in doc], columns=['token.dep_', 'token.head', 'token.children']) # indicizziamo tutto in un dataframe per recuperare i dati più velocemente\n",
    "    childrens = df.loc[VERB, 'token.children'] # prendiamo i termini che sono in dipendenza del verbo indicato\n",
    "    subj = None\n",
    "    obj = None\n",
    "    for c in childrens:\n",
    "        try:\n",
    "            c_dep = df.loc[str(c), 'token.dep_'] # prendiamo il valore della dipendenza\n",
    "            try:\n",
    "                c_dep = c_dep.iloc[0] # nel caso in cui la stessa parola compaia più volte nel dataframe prendiamo la prima occorrenza\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            # salviamo solo soggetto ed oggetto se presenti\n",
    "            if str(c_dep) == 'nsubj':\n",
    "                subj = str(c)\n",
    "            if str(c_dep) == 'dobj':\n",
    "                obj = str(c)\n",
    "        except KeyError:\n",
    "            # se ci sono più possibili triple nella stessa frase iteriamo su ognuna di esse\n",
    "            for sub_child in c:\n",
    "                c_dep = df.loc[str(sub_child), 'token.dep_'] # prendiamo il valore della dipendenza\n",
    "                try:\n",
    "                    c_dep = c_dep.iloc[0] # nel caso in cui la stessa parola compaia più volte nel dataframe ne prendiamo la prima occorrenza\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                # salviamo solo soggetto ed oggetto se presenti\n",
    "                if str(c_dep) == 'nsubj':\n",
    "                    subj = str(sub_child)\n",
    "                if str(c_dep) == 'dobj':\n",
    "                    obj = str(sub_child)\n",
    "            if subj and obj is not None:\n",
    "                fillers.append((subj, VERB, obj))\n",
    "                index.append(i)\n",
    "                # azzerando i valori di subj ed obj possiamo catturare anche le altre triple della frase, altrimenti ci sarebbero due triple uguali\n",
    "                subj = None\n",
    "                obj = None\n",
    "    if subj and obj is not None:\n",
    "        fillers.append((subj, VERB, obj))\n",
    "        index.append(i)\n",
    "\n",
    "print('Numero di triple estratte:', len(fillers)) # le triple complete (senza nessun campo None) sono 266\n",
    "fillers[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "    subj verb     obj\n2     we  get      it\n5   that  get  laughs\n6    who  get   kicks\n8     we  get   money\n9  movie  get    star",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subj</th>\n      <th>verb</th>\n      <th>obj</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>we</td>\n      <td>get</td>\n      <td>it</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>that</td>\n      <td>get</td>\n      <td>laughs</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>who</td>\n      <td>get</td>\n      <td>kicks</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>we</td>\n      <td>get</td>\n      <td>money</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>movie</td>\n      <td>get</td>\n      <td>star</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indicizziamo tutto in un dataframe per mostrare i risultati in maniera più organizzata\n",
    "df = pd.DataFrame(data=fillers, index=index, columns=['subj','verb', 'obj'])\n",
    "df[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Procediamo ora con la __disambiguazione__ dei fillers, in questo modo avremo il synset di _WordNet_ che ne rappresenta il senso. L'algoritmo di WSD utilizato è quello di _Lesk_, che ritorna il synset più adatto al termine oppure _None_ nel caso in cui non riesca ad ottenerne uno."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "    subj verb     obj                                  subj syn  \\\n2     we  get      it                                      None   \n5   that  get  laughs                                      None   \n6    who  get   kicks  Synset('world_health_organization.n.01')   \n8     we  get   money                                      None   \n9  movie  get    star                      Synset('movie.n.01')   \n\n                                 obj syn  \n2  Synset('information_technology.n.01')  \n5                    Synset('joke.n.01')  \n6                  Synset('recoil.n.01')  \n8                   Synset('money.n.03')  \n9           Synset('star_topology.n.01')  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subj</th>\n      <th>verb</th>\n      <th>obj</th>\n      <th>subj syn</th>\n      <th>obj syn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>we</td>\n      <td>get</td>\n      <td>it</td>\n      <td>None</td>\n      <td>Synset('information_technology.n.01')</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>that</td>\n      <td>get</td>\n      <td>laughs</td>\n      <td>None</td>\n      <td>Synset('joke.n.01')</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>who</td>\n      <td>get</td>\n      <td>kicks</td>\n      <td>Synset('world_health_organization.n.01')</td>\n      <td>Synset('recoil.n.01')</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>we</td>\n      <td>get</td>\n      <td>money</td>\n      <td>None</td>\n      <td>Synset('money.n.03')</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>movie</td>\n      <td>get</td>\n      <td>star</td>\n      <td>Synset('movie.n.01')</td>\n      <td>Synset('star_topology.n.01')</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_subj = []\n",
    "syn_obj = []\n",
    "for i, subj, obj in zip(df.index, df['subj'], df['obj']):\n",
    "    sent = sentences[i].split()\n",
    "    syn_subj.append(lesk(sent, subj))\n",
    "    syn_obj.append(lesk(sent, obj))\n",
    "df['subj syn'] = syn_subj\n",
    "df['obj syn'] = syn_obj\n",
    "df[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notiamo che in molte delle triple Lesk non riesce a trovare il senso del soggetto, mentre non ha grosse difficoltà a trovare quello dell'oggetto. Per avere un'idea contiamo il numero di elementi per cui è stato trovato il synset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di elementi non nulli fra i subj:   89\n",
      "Numero di elementi non nulli fra gli obj:  234\n",
      "Lunghezza totale delle colonne:   266\n"
     ]
    }
   ],
   "source": [
    "print('Numero di elementi non nulli fra i subj:  ', df['subj syn'].notnull().sum())\n",
    "print('Numero di elementi non nulli fra gli obj: ', df['obj syn'].notnull().sum())\n",
    "print('Lunghezza totale delle colonne:  ', len(df['subj syn']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recupero dei semantic types"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Passiamo adesso alla fase di estrazione dei __semantic types__. Li otteniamo accedendo a WordNet usando la funzione _lexname()_ presente in NLTK, che ritorna il super senso del termine."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "    subj verb     obj                                  subj syn  \\\n2     we  get      it                                      None   \n5   that  get  laughs                                      None   \n6    who  get   kicks  Synset('world_health_organization.n.01')   \n8     we  get   money                                      None   \n9  movie  get    star                      Synset('movie.n.01')   \n\n                                 obj syn    super sense subj  \\\n2  Synset('information_technology.n.01')                None   \n5                    Synset('joke.n.01')                None   \n6                  Synset('recoil.n.01')          noun.group   \n8                   Synset('money.n.03')                None   \n9           Synset('star_topology.n.01')  noun.communication   \n\n      super sense obj  \n2      noun.cognition  \n5  noun.communication  \n6          noun.event  \n8     noun.possession  \n9      noun.cognition  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subj</th>\n      <th>verb</th>\n      <th>obj</th>\n      <th>subj syn</th>\n      <th>obj syn</th>\n      <th>super sense subj</th>\n      <th>super sense obj</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>we</td>\n      <td>get</td>\n      <td>it</td>\n      <td>None</td>\n      <td>Synset('information_technology.n.01')</td>\n      <td>None</td>\n      <td>noun.cognition</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>that</td>\n      <td>get</td>\n      <td>laughs</td>\n      <td>None</td>\n      <td>Synset('joke.n.01')</td>\n      <td>None</td>\n      <td>noun.communication</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>who</td>\n      <td>get</td>\n      <td>kicks</td>\n      <td>Synset('world_health_organization.n.01')</td>\n      <td>Synset('recoil.n.01')</td>\n      <td>noun.group</td>\n      <td>noun.event</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>we</td>\n      <td>get</td>\n      <td>money</td>\n      <td>None</td>\n      <td>Synset('money.n.03')</td>\n      <td>None</td>\n      <td>noun.possession</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>movie</td>\n      <td>get</td>\n      <td>star</td>\n      <td>Synset('movie.n.01')</td>\n      <td>Synset('star_topology.n.01')</td>\n      <td>noun.communication</td>\n      <td>noun.cognition</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['super sense subj'] = df['subj syn'].apply(lambda x: x.lexname() if x is not None else None)\n",
    "df['super sense obj'] = df['obj syn'].apply(lambda x: x.lexname() if x is not None else None)\n",
    "df[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clusterizzazione dei semantic types\n",
    "\n",
    "Visualizziamo in maniera aggregata quali sono le categorie che compaiono con più frequenza fra i _semantic types_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prendiamo in esame la colonna dei _soggetti_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": "noun.substance        25\nnoun.person           16\nnoun.group             6\nadj.all                6\nnoun.communication     6\nnoun.cognition         5\nnoun.artifact          4\nnoun.quantity          4\nnoun.state             2\nverb.change            2\nnoun.event             2\nverb.motion            2\nnoun.Tops              2\nverb.competition       1\nnoun.food              1\nnoun.act               1\nnoun.time              1\nnoun.possession        1\nverb.consumption       1\nnoun.location          1\nName: super sense subj, dtype: int64"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df['super sense subj'].value_counts()))\n",
    "df['super sense subj'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora consideriamo quella degli _oggetti_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": "noun.communication    32\nnoun.cognition        25\nnoun.act              18\nnoun.person           16\nnoun.attribute        15\nnoun.state            14\nnoun.possession        9\nnoun.quantity          9\nnoun.artifact          9\nverb.communication     8\nadj.all                8\nverb.possession        7\nverb.social            6\nverb.cognition         6\nverb.perception        5\nnoun.location          5\nnoun.group             5\nverb.change            4\nverb.motion            4\nnoun.Tops              3\nnoun.time              3\nnoun.event             3\nnoun.phenomenon        2\nverb.competition       2\nnoun.feeling           2\nnoun.substance         2\nadv.all                2\nverb.body              2\nverb.consumption       2\nverb.creation          2\nverb.contact           1\nnoun.object            1\nnoun.animal            1\nverb.stative           1\nName: super sense obj, dtype: int64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df['super sense obj'].value_counts()))\n",
    "df['super sense obj'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Osservando le due distribuzioni si nota subito come la varianza di termini utilizzati sia molto maggiore per gli oggetti (34 categorie differenti).\n",
    "I soggetti invece sono in minor numero e concentrati nelle categorie _noun.substance_ e _noun.person_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Produciamo ora in output le frequenze delle triple _soggetto-verbo-oggetto_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "super sense subj    verb  super sense obj   \nnoun.group          get   noun.act              3\nnoun.substance      get   noun.possession       3\nnoun.person         get   noun.communication    2\nnoun.substance      get   noun.state            2\nnoun.person         get   verb.possession       2\n                          noun.attribute        2\n                          noun.artifact         2\n                          noun.act              2\nnoun.quantity       get   noun.feeling          2\nnoun.group          get   noun.attribute        2\nnoun.substance      get   noun.cognition        2\nnoun.communication  get   noun.cognition        2\nnoun.substance      get   noun.person           2\n                          verb.social           2\nnoun.person         get   noun.state            2\nnoun.substance      get   verb.cognition        1\nnoun.person         get   verb.perception       1\nverb.motion         get   verb.possession       1\nnoun.possession     get   noun.time             1\nverb.consumption    get   noun.communication    1\nverb.change         get   noun.quantity         1\n                          noun.act              1\nnoun.quantity       get   noun.communication    1\nnoun.time           get   verb.social           1\nnoun.substance      get   verb.stative          1\n                          noun.quantity         1\nnoun.state          get   noun.communication    1\nnoun.substance      get   adj.all               1\n                          noun.attribute        1\n                          verb.possession       1\n                          verb.communication    1\n                          noun.communication    1\n                          verb.body             1\nadj.all             get   adj.all               1\nnoun.person         get   verb.consumption      1\n                          noun.possession       1\nadj.all             get   noun.attribute        1\n                          noun.cognition        1\n                          noun.communication    1\n                          verb.communication    1\nnoun.Tops           get   noun.artifact         1\n                          noun.substance        1\nnoun.act            get   noun.attribute        1\nnoun.artifact       get   noun.event            1\n                          noun.possession       1\n                          noun.quantity         1\nnoun.cognition      get   adj.all               1\n                          noun.artifact         1\n                          noun.attribute        1\n                          noun.communication    1\n                          noun.possession       1\nnoun.communication  get   noun.attribute        1\n                          verb.creation         1\n                          verb.motion           1\nnoun.event          get   adj.all               1\n                          noun.phenomenon       1\nnoun.group          get   noun.event            1\nnoun.person         get   adj.all               1\nadj.all             get   noun.Tops             1\nverb.motion         get   verb.social           1\ndtype: int64"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['super sense subj', 'verb', 'super sense obj']).size().sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dalle frequenze delle triple notiamo risultati piuttosto bassi, si possono comunque individuare alcuni significati per il verbo selezionato (_get_), come quello di effettuare un'azione (_noun.group, get, noun.act_) e quello di possesso (_noun.substance, get, noun.possession_)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}