{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.wsd import lesk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Esercizio 1.4 - Teoria di Hanks\n",
    "\n",
    "La Teoria di Hanks propone di guardare al verbo ed alla sua valenza per trovare il significato di una frase.\n",
    "Prevede un processo composto di tre passi:\n",
    "1. estrazione dei _fillers_ del verbo\n",
    "2. recupero dei _semantic types_ a partire dai _fillers_\n",
    "3. clusterizzazione dei _semantic types_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per questo esercizio è stato utilizzato il corpus _movie reviews_ di NLTK, contenente recensioni di film."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Scegliamo il verbo su cui operare\n",
    "VERB = 'get'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estrazione dei fillers del verbo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dal corpus recuperiamo 1500 frasi contenenti il verbo _get_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di frasi recuperate:  1500\n"
     ]
    },
    {
     "data": {
      "text/plain": "array(['they get into an accident .',\n       \"now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem .\",\n       'okay , we get it .',\n       \"you're more likely to get a kick out of her work in halloween h20 .\",\n       'all this to get to the familiar yet offensive \" revelation \" that sexual deviants are not , indeed , monsters but everyday people like you and me .'],\n      dtype='<U887')"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentences(verb, corpus, num):\n",
    "    \"\"\"\n",
    "    Given a corpus and a verb return the number of senteneces specified in the limit.\n",
    "\n",
    "    :param verb: the verb as a string\n",
    "    :param corpus: the corpus in which the search must be done\n",
    "    :param num: the number of sentences to retrieve\n",
    "\n",
    "    :return: the list of sentences containing the verb\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    sentences = np.array([])\n",
    "    while len(sentences) < num:\n",
    "        review = corpus.raw(corpus.fileids()[i])\n",
    "        if verb in review.split():\n",
    "            for s in sent_tokenize(review): # le recensioni contengono più frasi, le dividiamo considerandole singolarmente\n",
    "                if verb in s.split():\n",
    "                    sentences = np.append(sentences, s)\n",
    "        i += 1\n",
    "    return sentences\n",
    "\n",
    "\n",
    "sentences = get_sentences(VERB, movie_reviews, 1500)\n",
    "print('Numero di frasi recuperate: ', len(sentences))\n",
    "sentences[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Effettuiamo __parsing__ e __disambiguazione__."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Partiamo dal __parsing__, il cui fine è quello di estrarre i _fillers_ del verbo dalle frasi.\n",
    "Essendo il verbo scelto un verbo transitivo, consideriamo l'arità pari a 2 (soggetto ed oggetto), otteniamo così delle triple _(subj, verb, obj)_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di triple estratte: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('we', 'get', 'it'),\n ('that', 'get', 'laughs'),\n ('who', 'get', 'kicks'),\n ('we', 'get', 'money'),\n ('movie', 'get', 'star')]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # usiamo la libreria SpaCy per ottenere le dipendenze delle frasi\n",
    "fillers = [] # inseriremo qui il soggetto e l'oggetto del verbo\n",
    "index = [] # teniamo traccia dell'indice della frase per poterla recuperare nelle operazioni successive (WSD)\n",
    "for i in range(len(sentences)):\n",
    "    doc = nlp(str(sentences[i]))\n",
    "    df = pd.DataFrame(data=[(token.dep_, token.head, [c for c in token.children]) for token in doc], index=[token.text for token in doc], columns=['token.dep_', 'token.head', 'token.children']) # indicizziamo tutto in un dataframe per recuperare i dati più velocemente\n",
    "    childrens = df.loc[VERB, 'token.children'] # prendiamo i termini che sono in dipendenza del verbo indicato\n",
    "    subj = None\n",
    "    obj = None\n",
    "    for c in childrens:\n",
    "        try:\n",
    "            c_dep = df.loc[str(c), 'token.dep_'] # prendiamo il valore della dipendenza\n",
    "            try:\n",
    "                c_dep = c_dep.iloc[0] # nel caso in cui la stessa parola compaia più volte nel dataframe prendiamo la prima occorrenza\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            # salviamo solo soggetto ed oggetto se presenti\n",
    "            if str(c_dep) == 'nsubj':\n",
    "                subj = str(c)\n",
    "            if str(c_dep) == 'dobj':\n",
    "                obj = str(c)\n",
    "        except KeyError:\n",
    "            # se ci sono più possibili triple nella stessa frase iteriamo su ognuna di esse\n",
    "            for sub_child in c:\n",
    "                c_dep = df.loc[str(sub_child), 'token.dep_'] # prendiamo il valore della dipendenza\n",
    "                try:\n",
    "                    c_dep = c_dep.iloc[0] # nel caso in cui la stessa parola compaia più volte nel dataframe ne prendiamo la prima occorrenza\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                # salviamo solo soggetto ed oggetto se presenti\n",
    "                if str(c_dep) == 'nsubj':\n",
    "                    subj = str(sub_child)\n",
    "                if str(c_dep) == 'dobj':\n",
    "                    obj = str(sub_child)\n",
    "            if subj and obj is not None:\n",
    "                fillers.append((subj, VERB, obj))\n",
    "                index.append(i)\n",
    "                # azzerando i valori di subj ed obj possiamo catturare anche le altre triple della frase, altrimenti ci sarebbero due triple uguali\n",
    "                subj = None\n",
    "                obj = None\n",
    "    if subj and obj is not None:\n",
    "        fillers.append((subj, VERB, obj))\n",
    "        index.append(i)\n",
    "\n",
    "print('Numero di triple estratte:', len(fillers)) # le triple complete (senza nessun campo None) sono 266\n",
    "fillers[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "    subj verb     obj\n2     we  get      it\n5   that  get  laughs\n6    who  get   kicks\n8     we  get   money\n9  movie  get    star",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subj</th>\n      <th>verb</th>\n      <th>obj</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>we</td>\n      <td>get</td>\n      <td>it</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>that</td>\n      <td>get</td>\n      <td>laughs</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>who</td>\n      <td>get</td>\n      <td>kicks</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>we</td>\n      <td>get</td>\n      <td>money</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>movie</td>\n      <td>get</td>\n      <td>star</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indicizziamo tutto in un dataframe per mostrare i risultati in maniera più organizzata\n",
    "df = pd.DataFrame(data=fillers, index=index, columns=['subj','verb', 'obj'])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Procediamo ora con la __disambiguazione__ dei fillers, in questo modo avremo il synset di _WordNet_ che ne rappresenta il senso. L'algoritmo di WSD utilizato è quello di _Lesk_, che ritorna il synset più adatto al termine oppure _None_ nel caso in cui non riesca ad ottenerne uno."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "    subj verb     obj                                  subj syn  \\\n2     we  get      it                                      None   \n5   that  get  laughs                                      None   \n6    who  get   kicks  Synset('world_health_organization.n.01')   \n8     we  get   money                                      None   \n9  movie  get    star                      Synset('movie.n.01')   \n\n                                 obj syn  \n2  Synset('information_technology.n.01')  \n5                    Synset('joke.n.01')  \n6                  Synset('recoil.n.01')  \n8                   Synset('money.n.03')  \n9           Synset('star_topology.n.01')  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subj</th>\n      <th>verb</th>\n      <th>obj</th>\n      <th>subj syn</th>\n      <th>obj syn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>we</td>\n      <td>get</td>\n      <td>it</td>\n      <td>None</td>\n      <td>Synset('information_technology.n.01')</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>that</td>\n      <td>get</td>\n      <td>laughs</td>\n      <td>None</td>\n      <td>Synset('joke.n.01')</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>who</td>\n      <td>get</td>\n      <td>kicks</td>\n      <td>Synset('world_health_organization.n.01')</td>\n      <td>Synset('recoil.n.01')</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>we</td>\n      <td>get</td>\n      <td>money</td>\n      <td>None</td>\n      <td>Synset('money.n.03')</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>movie</td>\n      <td>get</td>\n      <td>star</td>\n      <td>Synset('movie.n.01')</td>\n      <td>Synset('star_topology.n.01')</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_subj = []\n",
    "syn_obj = []\n",
    "for i, subj, obj in zip(df.index, df['subj'], df['obj']):\n",
    "    sent = sentences[i].split()\n",
    "    syn_subj.append(lesk(sent, subj))\n",
    "    syn_obj.append(lesk(sent, obj))\n",
    "df['subj syn'] = syn_subj\n",
    "df['obj syn'] = syn_obj\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notiamo che in molte delle triple Lesk non riesce a trovare il senso del soggetto, mentre non ha grosse difficoltà a trovare quello dell'oggetto. Per avere un'idea contiamo il numero di elementi per cui è stato trovato il synset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di elementi non nulli fra i subj:   123\n",
      "Numero di elementi non nulli fra gli obj:  335\n",
      "Lunghezza totale delle colonne:   384\n"
     ]
    }
   ],
   "source": [
    "print('Numero di elementi non nulli fra i subj:  ', df['subj syn'].notnull().sum())\n",
    "print('Numero di elementi non nulli fra gli obj: ', df['obj syn'].notnull().sum())\n",
    "print('Lunghezza totale delle colonne:  ', len(df['subj syn']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recupero dei semantic types"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Passiamo adesso alla fase di estrazione dei __semantic types__. Li otteniamo accedendo a WordNet ed usando la funzione _lexname()_ presente in NLTK, che ritorna il super senso del termine."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "    subj verb     obj                                  subj syn  \\\n2     we  get      it                                      None   \n5   that  get  laughs                                      None   \n6    who  get   kicks  Synset('world_health_organization.n.01')   \n8     we  get   money                                      None   \n9  movie  get    star                      Synset('movie.n.01')   \n\n                                 obj syn    super sense subj  \\\n2  Synset('information_technology.n.01')                None   \n5                    Synset('joke.n.01')                None   \n6                  Synset('recoil.n.01')          noun.group   \n8                   Synset('money.n.03')                None   \n9           Synset('star_topology.n.01')  noun.communication   \n\n      super sense obj  \n2      noun.cognition  \n5  noun.communication  \n6          noun.event  \n8     noun.possession  \n9      noun.cognition  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subj</th>\n      <th>verb</th>\n      <th>obj</th>\n      <th>subj syn</th>\n      <th>obj syn</th>\n      <th>super sense subj</th>\n      <th>super sense obj</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>we</td>\n      <td>get</td>\n      <td>it</td>\n      <td>None</td>\n      <td>Synset('information_technology.n.01')</td>\n      <td>None</td>\n      <td>noun.cognition</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>that</td>\n      <td>get</td>\n      <td>laughs</td>\n      <td>None</td>\n      <td>Synset('joke.n.01')</td>\n      <td>None</td>\n      <td>noun.communication</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>who</td>\n      <td>get</td>\n      <td>kicks</td>\n      <td>Synset('world_health_organization.n.01')</td>\n      <td>Synset('recoil.n.01')</td>\n      <td>noun.group</td>\n      <td>noun.event</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>we</td>\n      <td>get</td>\n      <td>money</td>\n      <td>None</td>\n      <td>Synset('money.n.03')</td>\n      <td>None</td>\n      <td>noun.possession</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>movie</td>\n      <td>get</td>\n      <td>star</td>\n      <td>Synset('movie.n.01')</td>\n      <td>Synset('star_topology.n.01')</td>\n      <td>noun.communication</td>\n      <td>noun.cognition</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['super sense subj'] = df['subj syn'].apply(lambda x: x.lexname() if x is not None else None)\n",
    "df['super sense obj'] = df['obj syn'].apply(lambda x: x.lexname() if x is not None else None)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clusterizzazione dei semantic types\n",
    "\n",
    "Visualizziamo in maniera aggregata quali sono le categorie che compaiono con più frequenza fra i _semantic types_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prendiamo in esame la colonna dei _soggetti_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di categorie per il soggetto:  24\n"
     ]
    },
    {
     "data": {
      "text/plain": "noun.substance        35\nnoun.person           19\nnoun.group            10\nadj.all               10\nnoun.communication     9\nnoun.cognition         8\nnoun.artifact          5\nnoun.quantity          4\nnoun.state             3\nnoun.location          2\nnoun.Tops              2\nverb.motion            2\nnoun.event             2\nverb.change            2\nverb.creation          1\nnoun.feeling           1\nverb.social            1\nnoun.time              1\nverb.competition       1\nnoun.act               1\nnoun.food              1\nnoun.possession        1\nverb.consumption       1\nadv.all                1\nName: super sense subj, dtype: int64"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Numero di categorie per il soggetto: ', len(df['super sense subj'].value_counts()))\n",
    "df['super sense subj'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora consideriamo quella degli _oggetti_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di categorie per l'oggetto:  35\n"
     ]
    },
    {
     "data": {
      "text/plain": "noun.communication    48\nnoun.cognition        34\nnoun.act              31\nnoun.person           26\nnoun.attribute        22\nnoun.state            21\nnoun.artifact         14\nnoun.possession       13\nnoun.quantity         11\nadj.all               10\nverb.possession        9\nverb.communication     8\nverb.cognition         8\nnoun.location          8\nverb.perception        7\nverb.social            6\nnoun.phenomenon        5\nnoun.event             5\nverb.change            5\nnoun.feeling           5\nverb.motion            5\nnoun.group             5\nnoun.time              4\nnoun.Tops              3\nverb.competition       3\nadv.all                3\nverb.stative           3\nverb.creation          3\nnoun.substance         2\nverb.body              2\nverb.consumption       2\nnoun.object            1\nverb.contact           1\nnoun.animal            1\nnoun.food              1\nName: super sense obj, dtype: int64"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Numero di categorie per l\\'oggetto: ', len(df['super sense obj'].value_counts()))\n",
    "df['super sense obj'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Osservando le due distribuzioni si nota subito come la varianza di termini utilizzati sia molto maggiore per gli oggetti (35 categorie differenti).\n",
    "I soggetti invece sono in minor numero e concentrati nelle categorie _noun.substance_ e _noun.person_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Produciamo ora in output le frequenze delle triple _soggetto-verbo-oggetto_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "super sense subj    verb  super sense obj\nnoun.substance      get   noun.state         4\n                          noun.cognition     4\nnoun.group          get   noun.act           3\nnoun.communication  get   noun.cognition     3\nadj.all             get   noun.attribute     3\n                                            ..\nnoun.group          get   noun.event         1\n                          noun.phenomenon    1\nnoun.location       get   noun.phenomenon    1\nnoun.person         get   adj.all            1\nverb.social         get   noun.time          1\nLength: 76, dtype: int64"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['super sense subj', 'verb', 'super sense obj']).size().sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le frequenze delle triple mostrano valori piuttosto bassi, risaltano comunque alcuni significati tipici del verbo selezionato (_get_), fra tutti:\n",
    "1. assumere uno stato (_noun.substance,get,noun.state_)\n",
    "2. effettuare un'azione (_noun.group, get, noun.act_)\n",
    "3. possesso (_noun.substance, get, noun.possession_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}