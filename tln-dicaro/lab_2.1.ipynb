{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Esercizio 2.1 - Text summarization estrattivo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per questo esercizio è stato scelto come corpus la pagina di Wikipedia sul _Natural Language Processing_ (<https://en.wikipedia.org/wiki/Natural_language_processing>).\n",
    "Il testo è stato recuperato utilizzando _SketchEngine_."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'obiettivo è quello di implementare un sistema di __text summarization estrattivo__ che riduca il numero di frasi del documento senza tralasciare informazioni importanti."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Carichiamo il corpus e creiamo un array con le singole frasi."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Natural language processing',\n       'This article is about natural language processing done by computers.',\n       'For the natural language processing done by the human brain, see Language processing in the brain.',\n       'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.',\n       'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.'],\n      dtype='<U423')"
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences = np.array([])\n",
    "for line in open('utils/wikipedia_nlp_page.txt', 'r').readlines():\n",
    "    all_sentences = np.append(all_sentences, sent_tokenize(line))\n",
    "all_sentences[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creiamo una copia dell'array per non modificare le frasi originali in fase di preprocessing (stopwards e punct removal ...) e poterle così recuperare intatte in fase di costruzione del riassunto."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "summary = np.copy(all_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Effettuiamo ora preprocessing sulle frasi dell'array copia."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "for line in open(\"utils/stop_words_FULL.txt\", 'r').readlines():\n",
    "    stopwords.append(line.rstrip('\\n'))\n",
    "stopwords = np.array(stopwords)\n",
    "\n",
    "\n",
    "def preprocessing(s):\n",
    "    \"\"\"\n",
    "    Do some preprocessing operations on the string.\n",
    "\n",
    "    :param s: the string\n",
    "\n",
    "    :return: the preprocessed string\n",
    "    \"\"\"\n",
    "    # Lowercasing\n",
    "    s = s.lower()\n",
    "    # Punct removal\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Stopword removal\n",
    "    s = ' '.join([word for word in s.split() if word not in stopwords])\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           sentences\n0                        natural language processing\n1      article natural language processing computers\n2  natural language processing human brain langua...\n3  natural language processing nlp subfield lingu...\n4  goal computer capable understanding contents d...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>natural language processing</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>article natural language processing computers</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>natural language processing human brain langua...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>natural language processing nlp subfield lingu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>goal computer capable understanding contents d...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_preprocessing = np.vectorize(preprocessing) # vettorizziamo la funzione in modo da applicarla a tutto l'array di frasi in maniera efficiente\n",
    "summary = pd.DataFrame(vect_preprocessing(summary), columns=['sentences'])\n",
    "summary.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per valutare le frasi e selezionarne le più importanti occorre avere un punteggio per ognuna di esse. Utilizziamo l'informazione statistica derivante dal testo, calcolando per ogni frase uno __score__ basato sulla frequenza delle singole parole nell'intero testo: ad ogni frase verrà quindi assegnato un punteggio pari alla somma degli score di tutte le parole che la compongono."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per prima cosa contiamo le __frequenze__ di tutte le parole nel testo, al netto delle _stopwords_ che vengono eliminate in fase di preprocessing, rendendo la computazione più snella."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "def get_frequencies(sents):\n",
    "    \"\"\"\n",
    "    Function to get the frequency of the words in the given sentences.\n",
    "\n",
    "    :param sents: a list of sentences\n",
    "\n",
    "    :return: a dictionary of words with their frequency\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    # prendiamo le singole parole\n",
    "    for s in sents:\n",
    "        words += word_tokenize(s)\n",
    "    # contiamo le parole in un dizionario\n",
    "    counts = dict.fromkeys(words, 0) # inizializziamo i conteggi di tutte le parole a 0\n",
    "    for w in words:\n",
    "        counts[w] += 1\n",
    "    for k in counts.keys():\n",
    "        counts[k] = counts[k] / max(counts.values()) # scaliamo i valori rendendoli tutti <=1\n",
    "    return counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "data": {
      "text/plain": "[('natural', 0.6212121212121212),\n ('language', 1.0),\n ('processing', 1.0),\n ('article', 0.07142857142857142),\n ('computers', 0.10714285714285714)]"
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies = get_frequencies(summary['sentences'])\n",
    "list(frequencies.items())[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sommiamo adesso gli score delle singole parole di ogni frase, in modo da avere un punteggio totale per ognuna di esse."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [
    {
     "data": {
      "text/plain": "                                             sentences     score\n3    natural language processing nlp subfield lingu...  9.857809\n192  interest increasingly abstract cognitive aspec...  9.419519\n233  chomskyan linguistics encourages investigation...  9.185223\n67   partofspeech tagging introduced hidden markov ...  8.351981\n9    premise symbolic nlp wellsummarized john searl...  8.313520",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentences</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>natural language processing nlp subfield lingu...</td>\n      <td>9.857809</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>interest increasingly abstract cognitive aspec...</td>\n      <td>9.419519</td>\n    </tr>\n    <tr>\n      <th>233</th>\n      <td>chomskyan linguistics encourages investigation...</td>\n      <td>9.185223</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>partofspeech tagging introduced hidden markov ...</td>\n      <td>8.351981</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>premise symbolic nlp wellsummarized john searl...</td>\n      <td>8.313520</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_score(s, freq):\n",
    "    \"\"\"\n",
    "    Calculate the score of a sentence based on the frequency of its words.\n",
    "\n",
    "    :param s: the sentence\n",
    "    :param freq: a dictionary with the words' frequencies score in the values\n",
    "\n",
    "    :return: the score of the sentence\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for w in s.split():\n",
    "        score += freq[w]\n",
    "    return score\n",
    "\n",
    "\n",
    "scores = []\n",
    "for s in summary['sentences']:\n",
    "    scores.append(compute_score(s, frequencies))\n",
    "summary['score'] = scores\n",
    "summary = summary.sort_values(by='score', ascending=False)\n",
    "summary.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ora che abbiamo tutti gli score delle frasi possiamo procedere alla composizione del riassunto.\n",
    "Per cominciare stabiliamo di quanto ridurre il documento."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il documento verrà riassunto in 94/314 frasi\n"
     ]
    }
   ],
   "source": [
    "compression_rate = 0.3\n",
    "document_len = len(all_sentences) # 314\n",
    "summary_len = int(document_len * compression_rate)\n",
    "print(f'Il documento verrà riassunto in {summary_len}/{document_len} frasi')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prendiamo quindi le prime $n$ frasi con lo score maggiore, le riordiniamo in base al loro ordine di uscita nel documento originale ed usiamo i loro indici per ricondurci alle frasi originali."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['For the natural language processing done by the human brain, see Language processing in the brain.',\n       'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.',\n       \"The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\",\n       'Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.',\n       '1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.'],\n      dtype='<U423')"
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prendiamo le frasi rilevanti e le ordiniamo\n",
    "indexes = summary.index[:summary_len].sort_values()\n",
    "# passiamo alle frasi originali\n",
    "summary = all_sentences[indexes]\n",
    "# stampiamo un'anteprima del riassunto\n",
    "summary[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Salviamo il riassunto completo in un file di testo per avere una visualizzazione migliore."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "np.savetxt(r'output/summary.txt', summary, fmt='%s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}