{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords as sw\n",
    "import string\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Esercizio 1.1 - Similarity\n",
    "\n",
    "Calcolo della similarità fra i termini del file *defs.csv* ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Caricamento del file *defs.csv* ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_defs = pd.read_csv(filepath_or_buffer=\"utils/defs.csv\", index_col=0).dropna()\n",
    "df_defs.reset_index(drop=True, inplace=True)\n",
    "NUM_DEFS = len(df_defs) # 30\n",
    "df_defs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fase di preprocessing (_lowercasing_, _punctuation removal_, _stopword removal_, _stemming_)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading stopwords list from file\n",
    "stopwords = []\n",
    "for line in open(\"utils/stop_words_FULL.txt\", 'r').readlines():\n",
    "    stopwords.append(line.rstrip('\\n'))\n",
    "stopwords = pd.Series(stopwords)\n",
    "\n",
    "# Initializing nltk.PorterStemmer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for c in df_defs.columns:\n",
    "    # Lowercasing\n",
    "    df_defs[c] = df_defs[c].str.lower()\n",
    "    # Punct removal\n",
    "    tmp = df_defs[c].apply(lambda x: str(x).translate(str.maketrans('', '', string.punctuation)))\n",
    "    df_defs[c] = tmp\n",
    "    # Stopword removal\n",
    "    tmp = df_defs[c].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stopwords.values]))\n",
    "    df_defs[c] = tmp\n",
    "    # Stemming\n",
    "    tmp = df_defs[c].apply(lambda x: ' '.join([ps.stem(word) for word in str(x).split()]))\n",
    "    df_defs[c] = tmp\n",
    "df_defs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calcolo della similarità fra le definizioni di ogni termine.\n",
    "\n",
    "La similarità viene calcolata come la media fra i valori medi delle _Bag of Words_ calcolati su tutte le combinazioni di definizioni. Ogni definizione avrà un certo numero medio di parole che sono presenti anche in altre definizioni, facendo la media fra tutti i valori si ottiene il punteggio di similarità del termine.\n",
    "\n",
    "Ci aspettiamo che i termini __concreti__ ('*Paper*', '*Sharpener*') abbiamo un valore di similarità maggiore rispetto a quelli __astratti__ ('*Courage*', '*Apprehension*')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bag_of_words(d, defs):\n",
    "    \"\"\"\n",
    "    Find the average intersection length between a definition and the others.\n",
    "\n",
    "    :param d: the definition\n",
    "    :param defs: the other definitions\n",
    "\n",
    "    :return: average length of the intersection\n",
    "    \"\"\"\n",
    "    d = set(d.split())\n",
    "    sum = 0\n",
    "    for other_d in defs:\n",
    "        bow = d.intersection(other_d.split()) # calcolo intersezione fra le due definizioni\n",
    "        sum += len(bow) / min(len(d), len(other_d.split())) # normalizzazione\n",
    "    return sum/NUM_DEFS\n",
    "\n",
    "\n",
    "def compute_sim(c):\n",
    "    \"\"\"\n",
    "    Find the average bow value between all the definitions of a term.\n",
    "\n",
    "    :param c: the term's column in the dataframe\n",
    "\n",
    "    :return: average bow value\n",
    "    \"\"\"\n",
    "    sum = 0\n",
    "    for i in range(NUM_DEFS):\n",
    "        sum += bag_of_words(df_defs[c][i], df_defs[c]) # calcoliamo la similarità di ogni definizione con a tutte le altre\n",
    "    return sum/NUM_DEFS # facciamo la media\n",
    "\n",
    "\n",
    "df_res = pd.DataFrame(index=['Generico', 'Specifico'], columns=['Astratto', 'Concreto'])\n",
    "df_res_terms = pd.DataFrame(index=['Generico', 'Specifico'], columns=['Astratto', 'Concreto'])\n",
    "for c, i, j in zip(df_defs.columns, [0,0,1,1], [0,1,0,1]):\n",
    "    df_res_terms.iloc[i,j] = f'{c}: {compute_sim(c)}'\n",
    "    df_res.iloc[i,j] = compute_sim(c)\n",
    "    # media delle medie delle intersezioni fra tutte le combinazioni di definizioni della colonna c\n",
    "    # per ogni definizione calcolo la bow media con tutte le altre defs, faccio la media delle medie ed ho la similarità\n",
    "print('--- Valori di similarità dei termini ---')\n",
    "print(df_res_terms, '\\n')\n",
    "print('--- Valore medio lungo le colonne ---')\n",
    "print(df_res.mean(axis=0), '\\n')\n",
    "print('--- Valore medio lungo le righe ---')\n",
    "print(df_res.mean(axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I risultati ottenuti confermano le ipotesi iniziali, in particolare:\n",
    "- se andiamo dall'__astratto al concreto__ abbiamo un valore __crescente__ di similarità\n",
    "- se andiamo dal __generico allo specifico__ abbiamo un valore __decrescente__ di similarità\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Esercizio 1.2 - Similarity explanation\n",
    "Spiegazione della similarità usando una lista delle parole più frequentemente usate nelle definizioni."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Per ogni termine viene calcolato:\n",
    " - il numero totale di paorle diverse usate nelle definizioni\n",
    " - una lista delle parole maggiormente usate (occorrono in almeno la metà delle definizioni)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for c in df_defs.columns:\n",
    "    words_count = df_defs[c].str.split(expand=True).stack().value_counts() # conta le occorrenze di ogni parola lungo tutta la colonna del termine\n",
    "    frequent_words = words_count[words_count > 15]  # prendiamo le parole che occorrono in 1/2 (15) delle definizioni, supponendo che ogni parola compaia al più una volta in ogni definizione\n",
    "    print(f'Word: \\'{c}\\'\\n'\n",
    "          f'Number of words: {len(words_count)}\\n'\n",
    "          f'List of frequent words: \\n{frequent_words}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dai dati ottenuti notiamo come per i termini '*Courage*', '*Paper*' ed '*Apprehension*' il numero delle parole usato sia molto alto, sintomo di una difficoltà nell'individuarli correttamente.\n",
    "\n",
    "Per '*Apprehension*' abbiamo il valore maggiore (57) ed infatti è il termine con il valore di similarità più basso. Questo rispecchia ciò che si era affermato in precedenza, infatti, essendo un termine astratto, è molto difficile da descrivere ed ognuno tende ad avere una propria rappresentazione.\n",
    "\n",
    "Discorso opposto per '*Sharpener*', il quale ha invece poche parole usate (28) ed è quello con la similarità maggiore. Essendo un oggetto fisico è più semplice descriverlo e si tende ad usare un linguaggio più uniforme.\n",
    "\n",
    "Fra le parole più usate nelle definizioni possiamo osservare che per gli oggetti __concreti__ si ha una concentrazione maggiore sugli stessi termini, in quanto le caratteristiche sono visibili ed è semplice descriverle.\n",
    "Questo non vale per gli oggetti __astratti__ i quali hanno una distribuzione delle parole diversa, basti guardare ad '*Apprehension*' che non ha nessuna parola frequente."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Esercizio 1.3 - WSI e pseudo-word evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementazione di un sistema di _Word Sense Induction_ che utilizza il metodo di _pseudo-word evaluation_ per disambiguare due termini."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "È stato creato un corpus di frasi usando la piattaforma SketchEngine, con la quale sono state estratte le frasi contenute in alcuni siti.\n",
    "Il corpus è stato pulito da alcuni tag e da alcune parti non utili (come le frasi dei footer dei siti) e sono stati corretti alcuni punti (come alcune frasi unite)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definiamo una funzione di preprocessing ed una funzione per estrarre il contesto dalle frasi.\n",
    "Il contesto viene estratto prendendo le 4 parole a dx ed a sx della parola target."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Loading stopwords list\n",
    "ita_stopwords = sw.words('italian')\n",
    "ita_stopwords.append('p') # to completely remove <p></p> tags from sentences\n",
    "ita_stopwords.append('homer')\n",
    "ita_stopwords.append('marge')\n",
    "ita_stopwords.append('bart')\n",
    "ita_stopwords.append('lisa')\n",
    "ita_stopwords.append('maggie')\n",
    "ita_stopwords.append('barney')\n",
    "ita_stopwords.append('boe')\n",
    "ita_stopwords.append('kent')\n",
    "ita_stopwords.append('brockman')\n",
    "\n",
    "# Initializing nltk.PorterStemmer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocessing(s):\n",
    "    \"\"\"\n",
    "    Do some preprocessing operations on the string.\n",
    "\n",
    "    :param s: the string\n",
    "\n",
    "    :return: the preprocessed string\n",
    "    \"\"\"\n",
    "    # Lowercasing\n",
    "    s = s.lower()\n",
    "    # Punct removal\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Stopword removal\n",
    "    s = ' '.join([word for word in s.split() if word not in ita_stopwords])\n",
    "    # Stemming\n",
    "    s = ' '.join([ps.stem(word) for word in s.split()])\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_context(w, s):\n",
    "    \"\"\"\n",
    "    Retrieve the context from a sentence, given the word w it takes the 4 words at its left and the 4 words at its right.\n",
    "\n",
    "    :param w: the target word\n",
    "    :param s: the sentence containing the word\n",
    "\n",
    "    :return: the context of the words\n",
    "    \"\"\"\n",
    "    s = s.split()\n",
    "    return s[s.index(w)-4:s.index(w)+5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creazione dei cluster di riferimento\n",
    "Scegliamo due parole $w_1, w_2$ di cui estrarremo i cluster per indurre i sensi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di frasi con 'birra': 35\n",
      "['cartello alimentato operai americani tedesca tv giappones carl',\n",
      " 'allora mettiamolo prova ama linguo robot amo birra',\n",
      " 'terribilment alto preoccupato tasso no momento dottori dite',\n",
      " 'accoglienza parlar leader cè là movimentariano birra permessa',\n",
      " 'felicità spensierata quando invec portata mano mhmh te']\n",
      "\n",
      "Numero di frasi con 'strada': 13\n",
      "['bacarozzi schifo uomo natura vittoria troy mcclure uccelli',\n",
      " 'banch apert anziani camminano impunement guarda ancora sbronzo',\n",
      " 'detto fidel castro dialogo prend nome quartier dedicata',\n",
      " 'clinton bob dole passeggiano tenendosi mano bob dole',\n",
      " 'carin apu lasciando attraversar fila paperel']\n"
     ]
    }
   ],
   "source": [
    "w1 = 'birra'#'tv'\n",
    "w2 = 'strada'# 'soldi'\n",
    "sentences_w1 = []\n",
    "sentences_w2 = []\n",
    "\n",
    "# carichiamo il corpus e procediamo ad estrarre i contesti delle parole\n",
    "'''\n",
    "nltk.download(\"movie_reviews\")\n",
    "from nltk.corpus import movie_reviews\n",
    "# more at https://www.nltk.org/nltk_data/\n",
    "'''\n",
    "for line in open(\"utils/cleaned_simpson_corpus.txt\", 'r').readlines():\n",
    "    if w1 in line.split():\n",
    "        try:\n",
    "            line = preprocessing(line)\n",
    "            line = get_context(ps.stem(w1), line)\n",
    "            line.remove(ps.stem(w1)) # todo se nella riga c'è più volte la parola essa viene considerata una sola volta. Si possono avere molti più contesti se si considerano tutte le occorrenze della parola usando line.count(w1)\n",
    "            line = ' '.join(word for word in line)\n",
    "            sentences_w1.append(line)\n",
    "        except ValueError as ve:\n",
    "            pass\n",
    "    elif w2 in line.split():\n",
    "        try:\n",
    "            line = preprocessing(line)\n",
    "            line = get_context(ps.stem(w2), line)\n",
    "            line.remove(ps.stem(w2))\n",
    "            line = ' '.join(word for word in line)\n",
    "            sentences_w2.append(line)\n",
    "        except ValueError as ve:\n",
    "            pass\n",
    "\n",
    "# stampiamo il numero di frasi trovate ed i primi 5 contesti della lista\n",
    "print(f'Numero di frasi con \\'{w1}\\':', len(sentences_w1))\n",
    "pprint(sentences_w1[0:5])\n",
    "print(f'\\nNumero di frasi con \\'{w2}\\':', len(sentences_w2))\n",
    "pprint(sentences_w2[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usando i contesti trovati, passiamo all'estrazione dei cluster. Sono stati sviluppati due metodi:\n",
    "- uno in cui viene creaata una __matrice di co-occorrenza__\n",
    "- l'altro utilizza invece l'__indice TF-IDF__ per avere delle features numeriche\n",
    "\n",
    "Entrambi proseguono applicando il __K-means__ per l'individuazione dei cluster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def co_occurence_matrix(sentences):\n",
    "    \"\"\"\n",
    "    Function that take a list of sentences and calculate co-occurrence matrix\n",
    "\n",
    "    :param sentences: list of sentences\n",
    "\n",
    "    :return: co-occurrence matrix as a Pandas dataframe, with words on rows and columns\n",
    "    \"\"\"\n",
    "    # Convert a collection of text documents to a matrix of token counts\n",
    "    cv = CountVectorizer(ngram_range=(1,1)) # verifichiamo la co-occorrenza di ogni singola parola con tutte le altre\n",
    "    # matrix of token counts\n",
    "    x = cv.fit_transform(sentences)\n",
    "    xc = (x.T * x) # matrix manipulation   # ---- https://www.quora.com/Whats-the-meaning-of-matrixs-transpose-multiplied-by-the-matrix-itself\n",
    "    xc.setdiag(0) # set the diagonals to be zeroes as it's pointless to be 1\n",
    "    names = cv.get_feature_names_out() # This are the entity names (i.e. keywords)\n",
    "    df = pd.DataFrame(data=xc.toarray(), columns=names, index=names)\n",
    "    return df\n",
    "\n",
    "def clustering_co_occ(sentences, clusters):\n",
    "    \"\"\"\n",
    "    Clustering of sentences using Co-Occurrence matrix and K-means algorithm.\n",
    "\n",
    "    :param sentences: list of sentences\n",
    "    :param clusters: list of clusters' names\n",
    "\n",
    "    :return: print the top terms of the two clusters\n",
    "    \"\"\"\n",
    "    # vectorize the text i.e. convert the strings to numeric features\n",
    "    X = co_occurence_matrix(sentences)\n",
    "\n",
    "    # cluster documents\n",
    "    true_k = len(clusters)  # numero dei cluster che vogliamo trovare\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=1000, n_init=1, random_state=27)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Prendiamo solo i primi termini\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = X.columns\n",
    "    '''\n",
    "    # print Top terms per cluster:\n",
    "    print(\"Top terms per cluster:\")\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind])\n",
    "    '''\n",
    "    df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n",
    "    return df\n",
    "\n",
    "\n",
    "def clustering_tf_idf(sentences, clusters):\n",
    "    \"\"\"\n",
    "    Clustering of sentences using Term Frequency and Inverse Document Frequency and K-means algorithm.\n",
    "\n",
    "    :param sentences: list of sentences\n",
    "    :param clusters: list of clusters' names\n",
    "\n",
    "    :return: print the top terms of the two clusters\n",
    "    \"\"\"\n",
    "    # vectorize the text i.e. convert the strings to numeric features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # cluster documents\n",
    "    true_k = len(clusters) # numero dei cluster che vogliamo trovare\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=1000, n_init=1, random_state=27)\n",
    "    model.fit(X)\n",
    "\n",
    "    # Prendiamo solo i primi termini\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    '''\n",
    "    # print Top terms per cluster:\n",
    "    print(\"Top terms per cluster:\")\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind])\n",
    "    '''\n",
    "    df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stampiamo i cluster trovati."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster di 'birra'\n",
      "     Cluster 0    Cluster 1\n",
      "0        duff        birra\n",
      "1       fuori          ama\n",
      "2       pizza     famiglia\n",
      "3          be  accoglienza\n",
      "4        cosa          dog\n",
      "..        ...          ...\n",
      "225        cè        fuori\n",
      "226    canzon         nome\n",
      "227  famiglia        pizza\n",
      "228       ama        chied\n",
      "229     birra      momento\n",
      "\n",
      "[230 rows x 2 columns] \n",
      "\n",
      "Cluster di 'strada'\n",
      "     Cluster 0    Cluster 1\n",
      "0        dole    pantaloni\n",
      "1         bob        mezzo\n",
      "2     sbronzo    abbassati\n",
      "3   bacarozzi        tizio\n",
      "4      guarda     camminar\n",
      "..        ...          ...\n",
      "95  spilungon  passeggiano\n",
      "96   camminar   pericolosa\n",
      "97  pantaloni   potrebbero\n",
      "98      mezzo        prego\n",
      "99  abbassati  lustrascarp\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9860/4280785254.py:47: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n",
      "/tmp/ipykernel_9860/4280785254.py:47: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n"
     ]
    }
   ],
   "source": [
    "# il tf-idf dovrebbe avere risultati migliori in quanto è una misura più raffinata (parte anch'essa dalla co-occ matrix e la elabora)\n",
    "# df_w1 = clustering_tf_idf(sentences_w1, ['Cluster 0', 'Cluster 1'])\n",
    "# df_w2 = clustering_tf_idf(sentences_w2, ['Cluster 0', 'Cluster 1'])\n",
    "\n",
    "df_w1 = clustering_co_occ(sentences_w1, ['Cluster 0', 'Cluster 1'])\n",
    "df_w2 = clustering_co_occ(sentences_w2, ['Cluster 0', 'Cluster 1'])\n",
    "print(f'Cluster di \\'{w1}\\'\\n', df_w1, '\\n')\n",
    "print(f'Cluster di \\'{w2}\\'\\n', df_w2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I cluster appena trovati rappresentano dei possibili sensi per le parole $w_1,w_2$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pseudo-word evaluation\n",
    "Applichiamo ora il metodo della __pseudo-word evaluation__, con il quale verifichiamo quanto cambiano i cluster associati alle parole se le sostituiamo all'interno del corpus con la loro concatenazione.\n",
    "Questo ci permette di verificare se i cluster sono corretti."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birra strada birrastrada\n",
      "\n",
      "Numero di frasi con 'birrastrada': 64\n"
     ]
    },
    {
     "data": {
      "text/plain": "['capisco be serv po',\n 'cartello alimentato operai americani tedesca tv giappones carl',\n 'far parlar cani spot ce dirà cow boy',\n 'rimastosenza vino carrello indicando dottor dice meccanico concorda',\n 'allora mettiamolo prova ama linguo robot amo birrastrada',\n 'terribilment alto preoccupato tasso no momento dottori dite',\n 'accoglienza parlar leader cè là movimentariano birrastrada permessa',\n 'reverendo lovejoy tenta corromperlo mmm birrastrada vuoi bel',\n 'felicità spensierata quando invec portata mano mhmh te',\n 'anchio coro alzando boccali salut stramitico grazi ragazzi']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concateniamo le due parole\n",
    "w = w1 + w2\n",
    "print(w1, w2, w)\n",
    "\n",
    "# i contesti sono uguali, li concateniamo soltanto\n",
    "# sentences_w = sentences_w1 + sentences_w2\n",
    "\n",
    "# ogni volta che una delle due parole viene trovata, si procede a sostituirla con la concatenazione e poi si processa la frase\n",
    "sentences_w = []\n",
    "for line in open('utils/cleaned_simpson_corpus.txt', 'r').readlines():\n",
    "    line = preprocessing(line)\n",
    "    if (w1 or w2) in line.split():\n",
    "        line = re.sub('|'.join(\"((?<=\\s)|(?<=^)){}((?=\\s)|(?=$))\".format(i) for i in [w1, w2]), w, line)\n",
    "        try:\n",
    "            line = get_context(ps.stem(w), line)\n",
    "            line.remove(ps.stem(w)) # todo se nella riga c'è più volte la parola essa viene considerata una sola volta. Si possono avere molti più contesti se si considerano tutte le occorrenze della parola usando line.count(w1)\n",
    "            line = ' '.join(word for word in line)\n",
    "            sentences_w.append(line)\n",
    "        except ValueError as ve:\n",
    "            pass\n",
    "print(f'\\nNumero di frasi con \\'{w}\\':', len(sentences_w))\n",
    "sentences_w[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9860/4280785254.py:47: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = pd.DataFrame(terms[order_centroids[:, :]].T, columns=clusters) # consideriamo solo le prime 30 parole del cluster\n"
     ]
    },
    {
     "data": {
      "text/plain": "       Cluster 0 Cluster 1 Cluster 2 Cluster 3\n0    birrastrada      duff        ce     tutta\n1           duff      lora    parlar    boccal\n2             tv  arrivata  prenderò       apu\n3        lattina    ammett       boy     mentr\n4          molto      sazi      cani   portano\n..           ...       ...       ...       ...\n390       parlar     ohahi    vorrei  famiglia\n391        mentr      dopo     molto   smither\n392      portano      sola     senza     molto\n393           ce        be       già       già\n394          apu      serv      duff      duff\n\n[395 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cluster 0</th>\n      <th>Cluster 1</th>\n      <th>Cluster 2</th>\n      <th>Cluster 3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>birrastrada</td>\n      <td>duff</td>\n      <td>ce</td>\n      <td>tutta</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>duff</td>\n      <td>lora</td>\n      <td>parlar</td>\n      <td>boccal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>tv</td>\n      <td>arrivata</td>\n      <td>prenderò</td>\n      <td>apu</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>lattina</td>\n      <td>ammett</td>\n      <td>boy</td>\n      <td>mentr</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>molto</td>\n      <td>sazi</td>\n      <td>cani</td>\n      <td>portano</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>parlar</td>\n      <td>ohahi</td>\n      <td>vorrei</td>\n      <td>famiglia</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>mentr</td>\n      <td>dopo</td>\n      <td>molto</td>\n      <td>smither</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>portano</td>\n      <td>sola</td>\n      <td>senza</td>\n      <td>molto</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>ce</td>\n      <td>be</td>\n      <td>già</td>\n      <td>già</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>apu</td>\n      <td>serv</td>\n      <td>duff</td>\n      <td>duff</td>\n    </tr>\n  </tbody>\n</table>\n<p>395 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w = clustering_co_occ(sentences_w, ['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3'])\n",
    "# df_w = clustering_tf_idf(sentences_w, ['Cluster 0', 'Cluster 1'])\n",
    "df_w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il cluster ottenuto è quello con la parola concatenata, andiamo adesso a verificare se esso contiene i termini dei cluster di riferimento delle parole $w_1,w_2$.\n",
    "Per valutare il sistema conteremo quanti elementi sono in comune, un buon sistema di WSI dovrebbe contenere un alto numero di termini."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 395\n",
      "Cluster 0: 229\n",
      "Cluster 0: 16\n",
      "\n",
      "\n",
      "Cluster 0: 395\n",
      "Cluster 1: 229\n",
      "Cluster 1: 16\n",
      "\n",
      "\n",
      "Cluster 1: 395\n",
      "Cluster 0: 229\n",
      "Cluster 0: 16\n",
      "\n",
      "\n",
      "Cluster 1: 395\n",
      "Cluster 1: 229\n",
      "Cluster 1: 16\n",
      "\n",
      "\n",
      "Cluster 2: 395\n",
      "Cluster 0: 229\n",
      "Cluster 0: 16\n",
      "\n",
      "\n",
      "Cluster 2: 395\n",
      "Cluster 1: 229\n",
      "Cluster 1: 16\n",
      "\n",
      "\n",
      "Cluster 3: 395\n",
      "Cluster 0: 229\n",
      "Cluster 0: 16\n",
      "\n",
      "\n",
      "Cluster 3: 395\n",
      "Cluster 1: 229\n",
      "Cluster 1: 16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in df_w.columns:\n",
    "    for (cw1, cw2) in zip(df_w1.columns, df_w2.columns):\n",
    "        diff1 = set(df_w[c]).intersection(df_w1[cw1])\n",
    "        diff2 = set(df_w[c]).intersection(df_w2[cw2])\n",
    "\n",
    "        # print(len(diff1), diff1)\n",
    "        # print(len(diff2), diff2)\n",
    "        # print('\\n', c, cw1, cw2)\n",
    "        # print(len(df_w), len(diff1), len(diff2))\n",
    "        print(f'{c}: {len(df_w)}\\n'\n",
    "              f'{cw1}: {len(diff1)}\\n'\n",
    "              f'{cw2}: {len(diff2)}\\n\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. trovati i cluster corretti dei sensi delle parole\n",
    "2. ora applicare la pseudo-words eval e calcolare i nuovi cluster\n",
    "3. vedere differenza fra i cluster nuovi e vecchi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}